{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsOnSYBqfoSL"
      },
      "source": [
        "# Evaluating Agents with Langfuse\n",
        "\n",
        "In this cookbook, we will learn how to **monitor the internal steps (traces) of the [OpenAI agent SDK](https://github.com/openai/openai-agents-python)** and **evaluate its performance** using [Langfuse](https://langfuse.com/docs).\n",
        "\n",
        "This guide covers **online** and **offline evaluation** metrics used by teams to bring agents to production fast and reliably. To learn more about evaluation strategies, check out this [blog post](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges).\n",
        "\n",
        "**Why AI agent Evaluation is important:**\n",
        "- Debugging issues when tasks fail or produce suboptimal results\n",
        "- Monitoring costs and performance in real-time\n",
        "- Improving reliability and safety through continuous feedback\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94-c-mbeVk4q"
      },
      "source": [
        "<div style=\"position: relative; padding-top: 69.85769728331177%;\">\n",
        "  <iframe\n",
        "    src=\"https://customer-xnej9vqjtgxpafyk.cloudflarestream.com/e335d35a0a762b76055f3e0b977f3380/iframe?muted=true&loop=true&autoplay=true&poster=https%3A%2F%2Fcustomer-xnej9vqjtgxpafyk.cloudflarestream.com%2Fe335d35a0a762b76055f3e0b977f3380%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600&controls=false\"\n",
        "    loading=\"lazy\"\n",
        "    style=\"border: white; position: absolute; top: 0; left: 0; height: 100%; width: 100%; border-radius: 10px;\"\n",
        "    allow=\"accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;\"\n",
        "    allowfullscreen=\"true\"\n",
        "  ></iframe>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzPbsmLrfoSN"
      },
      "source": [
        "## Step 0: Install the Required Libraries\n",
        "\n",
        "Below we install the `openai-agents` library (the OpenAI Agents SDK [link text](https://github.com/openai/openai-agents-python)), the `pydantic-ai[logfire]` OpenTelemetry instrumentation, `langfuse` and the Hugging Face `datasets` library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_EI_0ZfzfoSO",
        "outputId": "ace75429-9836-456e-98e7-b08df97f616e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-agents in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.0.17)\n",
            "Requirement already satisfied: griffe<2,>=1.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai-agents) (1.7.3)\n",
            "Requirement already satisfied: mcp<2,>=1.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai-agents) (1.9.4)\n",
            "Requirement already satisfied: openai>=1.81.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai-agents) (1.86.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai-agents) (2.11.5)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai-agents) (2.32.3)\n",
            "Requirement already satisfied: types-requests<3,>=2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai-agents) (2.32.4.20250611)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from openai-agents) (4.12.2)\n",
            "Requirement already satisfied: colorama>=0.4 in /home/codespace/.local/lib/python3.12/site-packages (from griffe<2,>=1.5.6->openai-agents) (0.4.6)\n",
            "Requirement already satisfied: anyio>=4.5 in /home/codespace/.local/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (4.9.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (0.4.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (2.9.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (2.3.6)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (0.47.0)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp<2,>=1.8.0->openai-agents) (0.34.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai>=1.81.0->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai>=1.81.0->openai-agents) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai>=1.81.0->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai>=1.81.0->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->mcp<2,>=1.8.0->openai-agents) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2,>=1.8.0->openai-agents) (0.14.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.8.0->openai-agents) (1.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp<2,>=1.8.0->openai-agents) (8.2.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nest_asyncio in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pydantic-ai[logfire] in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.2.16)\n",
            "Requirement already satisfied: pydantic-ai-slim==0.2.16 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.16)\n",
            "Requirement already satisfied: logfire>=3.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai[logfire]) (3.18.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.2)\n",
            "Requirement already satisfied: griffe>=1.3.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.7.3)\n",
            "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.34.1)\n",
            "Requirement already satisfied: pydantic-graph==0.2.16 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.16)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.11.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.4.1)\n",
            "Requirement already satisfied: fasta2a==0.2.16 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.16)\n",
            "Requirement already satisfied: anthropic>=0.52.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.54.0)\n",
            "Requirement already satisfied: boto3>=1.35.74 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.38.35)\n",
            "Requirement already satisfied: argcomplete>=3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.6.2)\n",
            "Requirement already satisfied: prompt-toolkit>=3 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.0.50)\n",
            "Requirement already satisfied: rich>=13 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (14.0.0)\n",
            "Requirement already satisfied: cohere>=5.13.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (5.15.0)\n",
            "Requirement already satisfied: pydantic-evals==0.2.16 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.16)\n",
            "Requirement already satisfied: google-genai>=1.15.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.20.0)\n",
            "Requirement already satisfied: groq>=0.15.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.27.0)\n",
            "Requirement already satisfied: mcp>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.9.4)\n",
            "Requirement already satisfied: mistralai>=1.2.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.8.2)\n",
            "Requirement already satisfied: openai>=1.75.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.86.0)\n",
            "Requirement already satisfied: google-auth>=2.36.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.40.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.32.3)\n",
            "Requirement already satisfied: starlette>0.29.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fasta2a==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.47.0)\n",
            "Requirement already satisfied: anyio>=0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (4.9.0)\n",
            "Requirement already satisfied: logfire-api>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-evals==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (6.0.2)\n",
            "Requirement already satisfied: executing>=2.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (2.2.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<1.35.0,>=1.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation>=0.41b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.35.0,>=1.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=4.23.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (5.29.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from logfire>=3.11.0->pydantic-ai[logfire]) (4.12.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.3.1)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.35 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.35.74->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.38.35)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.35.74->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.35.74->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.13.0)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.11.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.33.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.21.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.32.4.20250611)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (4.9.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-genai>=1.15.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (15.0.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /home/codespace/.local/lib/python3.12/site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.4.6)\n",
            "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.0.7)\n",
            "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.14.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.9.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.3.6)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.34.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from mistralai>=1.2.5->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai>=1.75.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<1.35.0,>=1.21.0->logfire>=3.11.0->pydantic-ai[logfire]) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<1.35.0,>=1.21.0->logfire>=3.11.0->pydantic-ai[logfire]) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<1.35.0,>=1.21.0->logfire>=3.11.0->pydantic-ai[logfire]) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.11.0->pydantic-ai[logfire]) (0.55b1)\n",
            "Requirement already satisfied: packaging>=18.0 in /home/codespace/.local/lib/python3.12/site-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.11.0->pydantic-ai[logfire]) (24.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.11.0->pydantic-ai[logfire]) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.2.16->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->mistralai>=1.2.5->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (0.33.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp>=1.9.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (8.2.1)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (2024.6.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.2.16->pydantic-ai[logfire]) (1.1.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: langfuse in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.0.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (2.2.1)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /home/codespace/.local/lib/python3.12/site-packages (from langfuse) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.33.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (1.34.1)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /home/codespace/.local/lib/python3.12/site-packages (from langfuse) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (2.11.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from langfuse) (2.32.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langfuse) (1.17.2)\n",
            "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (4.9.0)\n",
            "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.7)\n",
            "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (4.12.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.73.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse) (0.55b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langfuse) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langfuse) (2.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<1.0,>=0.15.4->langfuse) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: datasets in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai-agents\n",
        "%pip install nest_asyncio\n",
        "%pip install pydantic-ai[logfire]\n",
        "%pip install langfuse\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHRsxz1VfoSP"
      },
      "source": [
        "## Step 1: Instrument Your Agent\n",
        "\n",
        "In this notebook, we will use [Langfuse](https://langfuse.com/) to trace, debug and evaluate our agent.\n",
        "\n",
        "**Note:** If you are using LlamaIndex or LangGraph, you can find documentation on instrumenting them [here](https://langfuse.com/docs/integrations/llama-index/workflows) and [here](https://langfuse.com/docs/integrations/langchain/example-python-langgraph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZnxtWx9foSP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "LANGFUSE_AUTH = base64.b64encode(\n",
        "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
        ").decode()\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# Set your OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KQjuJNHkfoSP"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "# Create a TracerProvider for OpenTelemetry\n",
        "trace_provider = TracerProvider()\n",
        "\n",
        "# Add a SimpleSpanProcessor with the OTLPSpanExporter to send traces\n",
        "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
        "\n",
        "# Set the global default tracer provider\n",
        "from opentelemetry import trace\n",
        "trace.set_tracer_provider(trace_provider)\n",
        "tracer = trace.get_tracer(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWr-MQY7hKdM"
      },
      "source": [
        "Pydantic Logfire offers an instrumentation for the OpenAi Agent SDK. We use this to send traces to the [Langfuse OpenTelemetry Backend](https://langfuse.com/docs/opentelemetry/get-started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "td11AsCShBxA"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MQoskgIhCQi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to export batch code: 401, reason: {\"message\":\"Invalid credentials. Confirm that you've configured the correct host.\"}\n"
          ]
        }
      ],
      "source": [
        "import logfire\n",
        "\n",
        "# Configure logfire instrumentation.\n",
        "logfire.configure(\n",
        "    service_name='my_agent_service',\n",
        "\n",
        "    send_to_logfire=False,\n",
        ")\n",
        "# This method automatically patches the OpenAI Agents SDK to send logs via OTLP to Langfuse.\n",
        "logfire.instrument_openai_agents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uulS5iGHfoSP"
      },
      "source": [
        "## Step 2: Test Your Instrumentation\n",
        "\n",
        "Here is a simple Q&A agent. We run it to confirm that the instrumentation is working correctly. If everything is set up correctly, you will see logs/spans in your observability dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcyynS9CfoSP",
        "outputId": "5e8eb8c2-bcff-4149-ff8b-21d706c1d25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "08:56:03.865 OpenAI Agents trace: Agent workflow\n",
            "08:56:03.867   Agent run: 'Assistant'\n",
            "08:56:03.877     Responses API with 'gpt-4o'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error getting response: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}. (request_id: req_6e3178e28750b9ebe64c9df67d6962f4)\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     13\u001b[39m loop = asyncio.get_running_loop()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop.create_task(main())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      5\u001b[39m     agent = Agent(\n\u001b[32m      6\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mAssistant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m         instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a senior software engineer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[33m\"\u001b[39m\u001b[33mTell me why it is important to evaluate AI agents.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:219\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    214\u001b[39m logger.debug(\n\u001b[32m    215\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    221\u001b[39m             starting_agent,\n\u001b[32m    222\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    223\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    224\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    225\u001b[39m             context_wrapper,\n\u001b[32m    226\u001b[39m         ),\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    228\u001b[39m             agent=current_agent,\n\u001b[32m    229\u001b[39m             all_tools=all_tools,\n\u001b[32m    230\u001b[39m             original_input=original_input,\n\u001b[32m    231\u001b[39m             generated_items=generated_items,\n\u001b[32m    232\u001b[39m             hooks=hooks,\n\u001b[32m    233\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    234\u001b[39m             run_config=run_config,\n\u001b[32m    235\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    236\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    237\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    242\u001b[39m         agent=current_agent,\n\u001b[32m    243\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    252\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:787\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    785\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    788\u001b[39m     agent,\n\u001b[32m    789\u001b[39m     system_prompt,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    791\u001b[39m     output_schema,\n\u001b[32m    792\u001b[39m     all_tools,\n\u001b[32m    793\u001b[39m     handoffs,\n\u001b[32m    794\u001b[39m     context_wrapper,\n\u001b[32m    795\u001b[39m     run_config,\n\u001b[32m    796\u001b[39m     tool_use_tracker,\n\u001b[32m    797\u001b[39m     previous_response_id,\n\u001b[32m    798\u001b[39m )\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    801\u001b[39m     agent=agent,\n\u001b[32m    802\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    812\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:946\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    943\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    944\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    947\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    948\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    949\u001b[39m     model_settings=model_settings,\n\u001b[32m    950\u001b[39m     tools=all_tools,\n\u001b[32m    951\u001b[39m     output_schema=output_schema,\n\u001b[32m    952\u001b[39m     handoffs=handoffs,\n\u001b[32m    953\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    954\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    955\u001b[39m     ),\n\u001b[32m    956\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    957\u001b[39m )\n\u001b[32m    959\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/models/openai_responses.py:80\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     81\u001b[39m             system_instructions,\n\u001b[32m     82\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     83\u001b[39m             model_settings,\n\u001b[32m     84\u001b[39m             tools,\n\u001b[32m     85\u001b[39m             output_schema,\n\u001b[32m     86\u001b[39m             handoffs,\n\u001b[32m     87\u001b[39m             previous_response_id,\n\u001b[32m     88\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     89\u001b[39m         )\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     92\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/models/openai_responses.py:248\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    238\u001b[39m     logger.debug(\n\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    249\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    250\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    251\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    252\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    253\u001b[39m     include=converted_tools.includes,\n\u001b[32m    254\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    255\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    256\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    257\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    258\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    259\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    260\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    261\u001b[39m     stream=stream,\n\u001b[32m    262\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    263\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    264\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    265\u001b[39m     text=response_format,\n\u001b[32m    266\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    267\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    268\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    269\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/responses/responses.py:1898\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, input, model, background, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1867\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1868\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1869\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1896\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1897\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1899\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1900\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1901\u001b[39m             {\n\u001b[32m   1902\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1903\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1904\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   1905\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   1906\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   1907\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   1908\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1909\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1910\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   1911\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   1912\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1913\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1914\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1915\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1916\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   1917\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1918\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1919\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1920\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   1921\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1922\u001b[39m             },\n\u001b[32m   1923\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   1924\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1925\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   1926\u001b[39m         ),\n\u001b[32m   1927\u001b[39m         options=make_request_options(\n\u001b[32m   1928\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1929\u001b[39m         ),\n\u001b[32m   1930\u001b[39m         cast_to=Response,\n\u001b[32m   1931\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1932\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   1933\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1736\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1743\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1745\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1552\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1554\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1557\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[non-fatal] Tracing client error 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Incorrect API key provided: sk-proj-.... You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
            "    \"type\": \"invalid_request_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": \"invalid_api_key\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are a senior software engineer\",\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(agent, \"Tell me why it is important to evaluate AI agents.\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPLt1hRkfoSQ"
      },
      "source": [
        "Check your [Langfuse Traces Dashboard](https://cloud.langfuse.com/traces) to confirm that the spans and logs have been recorded.\n",
        "\n",
        "Example trace in Langfuse:\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/first-example-trace.png)\n",
        "\n",
        "_[Link to the trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0195948781a9f0d78fd5e067154aa508?timestamp=2025-03-14T12%3A01%3A03.401Z&observation=64bcac3cb82d04e9)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onjMD-ZJfoSQ"
      },
      "source": [
        "## Step 3: Observe and Evaluate a More Complex Agent\n",
        "\n",
        "Now that you have confirmed your instrumentation works, let's try a more complex query so we can see how advanced metrics (token usage, latency, costs, etc.) are tracked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qdMJh9KfoSQ",
        "outputId": "04136f13-51b0-4938-afa6-ade016c01aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:33:30.839 OpenAI Agents trace: Agent workflow\n",
            "13:33:30.840   Agent run: 'Hello world'\n",
            "13:33:30.842     Responses API with 'gpt-4o'\n",
            "13:33:31.822     Function: get_weather\n",
            "13:33:31.825     Responses API with 'gpt-4o'\n",
            "The weather in Berlin is currently sunny.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner, function_tool\n",
        "\n",
        "# Example function tool.\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Hello world\",\n",
        "    instructions=\"You are a helpful agent.\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    result = await Runner.run(agent, input=\"What's the weather in Berlin?\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjkhTgLWfoSQ"
      },
      "source": [
        "### Trace Structure\n",
        "\n",
        "Langfuse records a **trace** that contains **spans**, which represent each step of your agentâ€™s logic. Here, the trace contains the overall agent run and sub-spans for:\n",
        "- The tool call (get_weather)\n",
        "- The LLM calls (Responses API with 'gpt-4o')\n",
        "\n",
        "You can inspect these to see precisely where time is spent, how many tokens are used, and so on:\n",
        "\n",
        "![Trace tree in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/trace-tree.png)\n",
        "\n",
        "_[Link to the trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019594b5b9a27c5d497b13be71e7f255?timestamp=2025-03-14T12%3A51%3A32.386Z&display=preview&observation=6374a3c96baf831d)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHZAkQuefoSQ"
      },
      "source": [
        "## Online Evaluation\n",
        "\n",
        "Online Evaluation refers to evaluating the agent in a live, real-world environment, i.e. during actual usage in production. This involves monitoring the agentâ€™s performance on real user interactions and analyzing outcomes continuously.\n",
        "\n",
        "We have written down a guide on different evaluation techniques [here](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges).\n",
        "\n",
        "### Common Metrics to Track in Production\n",
        "\n",
        "1. **Costs** â€” The instrumentation captures token usage, which you can transform into approximate costs by assigning a price per token.\n",
        "2. **Latency** â€” Observe the time it takes to complete each step, or the entire run.\n",
        "3. **User Feedback** â€” Users can provide direct feedback (thumbs up/down) to help refine or correct the agent.\n",
        "4. **LLM-as-a-Judge** â€” Use a separate LLM to evaluate your agentâ€™s output in near real-time (e.g., checking for toxicity or correctness).\n",
        "\n",
        "Below, we show examples of these metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHMvJ1QlfoSQ"
      },
      "source": [
        "#### 1. Costs\n",
        "\n",
        "Below is a screenshot showing usage for `gpt-4o` calls. This is useful to see costly steps and optimize your agent.\n",
        "\n",
        "![Costs](https://langfuse.com/images/cookbook/integration_openai-agents/gpt-4o-costs.png)\n",
        "\n",
        "_[Link to the trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019594b5b9a27c5d497b13be71e7f255?timestamp=2025-03-14T12%3A51%3A32.386Z&display=preview&observation=6374a3c96baf831d)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz0y9mn7foSQ"
      },
      "source": [
        "#### 2. Latency\n",
        "\n",
        "We can also see how long it took to complete each step. In the example below, the entire run took 7 seconds, which you can break down by step. This helps you identify bottlenecks and optimize your agent.\n",
        "\n",
        "![Latency](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-latency.png)\n",
        "\n",
        "_[Link to the trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019594b5b9a27c5d497b13be71e7f255?timestamp=2025-03-14T12%3A51%3A32.386Z&display=timeline&observation=b12967a01b3f8bcb)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByq31MzfoSQ"
      },
      "source": [
        "#### 3. Additional Attributes\n",
        "\n",
        "Opentelemetry lets you attach a set of attributes to all spans by setting [`set_attribute`](https://opentelemetry.io/docs/languages/python/instrumentation/#add-attributes-to-a-span). This allows you to set properties like a Langfuse Session ID, to group traces into Langfuse Sessions or a User ID, to assign traces to a specific user. You can find a list of all supported attributes in the [here](/docs/opentelemetry/get-started#property-mapping).\n",
        "\n",
        "In this example, we pass a [user_id](https://langfuse.com/docs/tracing-features/users), [session_id](https://langfuse.com/docs/tracing-features/sessions) and [trace_tags](https://langfuse.com/docs/tracing-features/tags) to Langfuse. You can also use the span attribute `input.value` and `output.value` to set the trace level input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaSQNrgyfoSR"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "trace_provider = TracerProvider()\n",
        "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
        "\n",
        "# Sets the global default tracer provider\n",
        "from opentelemetry import trace\n",
        "trace.set_tracer_provider(trace_provider)\n",
        "\n",
        "# Creates a tracer from the global tracer provider\n",
        "tracer = trace.get_tracer(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5O02Ren3Kmu",
        "outputId": "63a0020d-c873-46c6-d159-3a045ab681f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:34:49.654 OpenAI Agents trace: Agent workflow\n",
            "13:34:49.655   Agent run: 'Assistant'\n",
            "13:34:49.657     Responses API with 'gpt-4o'\n",
            "AI agent evaluation is crucial for several reasons:\n",
            "\n",
            "1. **Performance Verification**: It ensures that the AI agent performs its intended tasks effectively and meets specific criteria or benchmarks.\n",
            "\n",
            "2. **Safety and Reliability**: Evaluation helps identify and mitigate risks, ensuring that the AI operates safely and reliably in real-world situations.\n",
            "\n",
            "3. **Continuous Improvement**: Analyzing performance data allows developers to refine and enhance the AI, leading to better outcomes and more efficient systems.\n",
            "\n",
            "4. **Transparency and Accountability**: Thorough evaluation provides transparency into how decisions are made by the AI, which is essential for accountability, especially in sensitive applications.\n",
            "\n",
            "5. **Bias and Fairness**: Evaluating AI systems helps detect and address potential biases, ensuring fair treatment of all users and stakeholders.\n",
            "\n",
            "6. **Compliance**: It ensures adherence to regulations and industry standards, which is critical for legal and ethical compliance.\n",
            "\n",
            "7. **User Trust**: A well-evaluated AI fosters trust among users, stakeholders, and the public, as they can be confident in its capabilities and limitations.\n",
            "\n",
            "8. **Resource Allocation**: Evaluation helps determine if the AI is using resources efficiently, which can be crucial for cost management and scalability.\n"
          ]
        }
      ],
      "source": [
        "input_query = \"Why is AI agent evaluation important?\"\n",
        "\n",
        "with tracer.start_as_current_span(\"OpenAI-Agent-Trace\") as span:\n",
        "    span.set_attribute(\"langfuse.user.id\", \"user-12345\")\n",
        "    span.set_attribute(\"langfuse.session.id\", \"my-agent-session\")\n",
        "    span.set_attribute(\"langfuse.tags\", [\"staging\", \"demo\", \"OpenAI Agent SDK\"])\n",
        "\n",
        "    async def main(input_query):\n",
        "        agent = Agent(\n",
        "            name = \"Assistant\",\n",
        "            instructions = \"You are a helpful assistant.\",\n",
        "        )\n",
        "\n",
        "        result = await Runner.run(agent, input_query)\n",
        "        print(result.final_output)\n",
        "        return result\n",
        "\n",
        "    result = await main(input_query)\n",
        "\n",
        "    # Add input and output values to parent trace\n",
        "    span.set_attribute(\"input.value\", input_query)\n",
        "    span.set_attribute(\"output.value\", result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEPeGdAafoSR"
      },
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-sdk-custom-attributes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtKiK62HfoSR"
      },
      "source": [
        "#### 4. User Feedback\n",
        "\n",
        "If your agent is embedded into a user interface, you can record direct user feedback (like a thumbs-up/down in a chat UI). Below is an example using `IPython.display` for simple feedback mechanism.\n",
        "\n",
        "In the code snippet below, when a user sends a chat message, we capture the OpenTelemetry trace ID. If the user likes/dislikes the last answer, we attach a score to the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "8b10018448324153af2ee1f9bd83d140",
            "cebcce63ea37474ca10f1828105ca2e6",
            "9153dfceabff450ead31493c3c518d4c",
            "5a2b1d2255a34a7597b263755eaa14b3",
            "c8b3aa3aeec046ef8acfab640c2dee17",
            "ee1b1596e6ec42029fbf8b711c0fc41a",
            "ecd5521cdbc34eb7a866b4b2094fd500",
            "df007d6320cb4198a6dbf58485980394"
          ]
        },
        "id": "YI9siKKKfoSR",
        "outputId": "3eb086d0-4277-4cdd-966f-10b3c12272a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: What is Langfuse?\n",
            "13:54:41.574 OpenAI Agents trace: Agent workflow\n",
            "13:54:41.575   Agent run: 'WebSearchAgent'\n",
            "13:54:41.577     Responses API with 'gpt-4o'\n",
            "Langfuse is an open-source engineering platform designed to enhance the development, monitoring, and optimization of Large Language Model (LLM) applications. It offers a suite of tools that provide observability, prompt management, evaluations, and metrics, facilitating the debugging and improvement of LLM-based solutions. ([toolkitly.com](https://www.toolkitly.com/langfuse?utm_source=openai))\n",
            "\n",
            "**Key Features of Langfuse:**\n",
            "\n",
            "- **LLM Observability:** Langfuse enables developers to monitor and analyze the performance of language models by tracking API calls, user inputs, prompts, and outputs. This observability aids in understanding model behavior and identifying areas for improvement. ([toolkitly.com](https://www.toolkitly.com/langfuse?utm_source=openai))\n",
            "\n",
            "- **Prompt Management:** The platform provides tools for managing, versioning, and deploying prompts directly within Langfuse. This feature allows for efficient organization and refinement of prompts to optimize model responses. ([toolkitly.com](https://www.toolkitly.com/langfuse?utm_source=openai))\n",
            "\n",
            "- **Evaluations and Metrics:** Langfuse offers capabilities to collect and calculate scores for LLM completions, run model-based evaluations, and gather user feedback. It also tracks key metrics such as cost, latency, and quality, providing insights through dashboards and data exports. ([toolkitly.com](https://www.toolkitly.com/langfuse?utm_source=openai))\n",
            "\n",
            "- **Playground Environment:** The platform includes a playground where users can interactively experiment with different models and prompts, facilitating prompt engineering and testing. ([toolkitly.com](https://www.toolkitly.com/langfuse?utm_source=openai))\n",
            "\n",
            "- **Integration Capabilities:** Langfuse integrates seamlessly with various tools and frameworks, including LlamaIndex, LangChain, OpenAI SDK, LiteLLM, and more, enhancing its functionality and allowing for the development of complex applications. ([toolerific.ai](https://toolerific.ai/ai-tools/opensource/langfuse-langfuse?utm_source=openai))\n",
            "\n",
            "- **Open Source and Self-Hosting:** Being open-source, Langfuse allows developers to customize and extend the platform according to their specific needs. It can be self-hosted, providing full control over infrastructure and data. ([vafion.com](https://www.vafion.com/blog/unlocking-power-language-models-langfuse/?utm_source=openai))\n",
            "\n",
            "Langfuse is particularly valuable for developers and researchers working with LLMs, offering a comprehensive set of tools to improve the performance and reliability of LLM applications. Its flexibility, integration capabilities, and open-source nature make it a robust choice for those seeking to enhance their LLM projects. \n",
            "How did you like the agent response?\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b10018448324153af2ee1f9bd83d140",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Button(description='ðŸ‘', icon='thumbs-up', style=ButtonStyle()), Button(description='ðŸ‘Ž', icon='tâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scored the trace in Langfuse\n"
          ]
        }
      ],
      "source": [
        "%pip install -q ipywidgets\n",
        "%pip install -q jupyterlab-widgets\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from agents import Agent, Runner, WebSearchTool\n",
        "from opentelemetry.trace import format_trace_id\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "# Define your agent with the web search tool\n",
        "agent = Agent(\n",
        "    name=\"WebSearchAgent\",\n",
        "    instructions=\"You are an agent that can search the web.\",\n",
        "    tools=[WebSearchTool()]\n",
        ")\n",
        "\n",
        "formatted_trace_id = None  # We'll store the current trace_id globally for demonstration\n",
        "\n",
        "def on_feedback(button):\n",
        "    if button.icon == \"thumbs-up\":\n",
        "      langfuse.score(\n",
        "            value=1,\n",
        "            name=\"user-feedback\",\n",
        "            comment=\"The user gave this response a thumbs up\",\n",
        "            trace_id=formatted_trace_id\n",
        "        )\n",
        "    elif button.icon == \"thumbs-down\":\n",
        "      langfuse.score(\n",
        "            value=0,\n",
        "            name=\"user-feedback\",\n",
        "            comment=\"The user gave this response a thumbs down\",\n",
        "            trace_id=formatted_trace_id\n",
        "        )\n",
        "    print(\"Scored the trace in Langfuse\")\n",
        "\n",
        "user_input = input(\"Enter your question: \")\n",
        "\n",
        "# Run agent\n",
        "with trace.get_tracer(__name__).start_as_current_span(\"OpenAI-Agent-Trace\") as span:\n",
        "\n",
        "    # Run your agent with a query\n",
        "    result = Runner.run_sync(agent, user_input)\n",
        "    print(result.final_output)\n",
        "\n",
        "    current_span = trace.get_current_span()\n",
        "    span_context = current_span.get_span_context()\n",
        "    trace_id = span_context.trace_id\n",
        "    formatted_trace_id = str(format_trace_id(trace_id))\n",
        "    langfuse.trace(id=formatted_trace_id, input=user_input, output=result.final_output)\n",
        "\n",
        "# Get feedback\n",
        "print(\"How did you like the agent response?\")\n",
        "\n",
        "thumbs_up = widgets.Button(description=\"ðŸ‘\", icon=\"thumbs-up\")\n",
        "thumbs_down = widgets.Button(description=\"ðŸ‘Ž\", icon=\"thumbs-down\")\n",
        "\n",
        "thumbs_up.on_click(on_feedback)\n",
        "thumbs_down.on_click(on_feedback)\n",
        "\n",
        "display(widgets.HBox([thumbs_up, thumbs_down]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiemuS7YfoSR"
      },
      "source": [
        "User feedback is then captured in Langfuse:\n",
        "\n",
        "![User feedback is being captured in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/open-ai-agent-user-feedback.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29KsI9xcfoSR"
      },
      "source": [
        "#### 5. LLM-as-a-Judge\n",
        "\n",
        "LLM-as-a-Judge is another way to automatically evaluate your agent's output. You can set up a separate LLM call to gauge the outputâ€™s correctness, toxicity, style, or any other criteria you care about.\n",
        "\n",
        "**Workflow**:\n",
        "1. You define an **Evaluation Template**, e.g., \"Check if the text is toxic.\"\n",
        "2. You set a model that is used as judge-model; in this case `gpt-4o-mini`.\n",
        "2. Each time your agent generates output, you pass that output to your \"judge\" LLM with the template.\n",
        "3. The judge LLM responds with a rating or label that you log to your observability tool.\n",
        "\n",
        "Example from Langfuse:\n",
        "\n",
        "![LLM-as-a-Judge Evaluation Template](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator-template.png)\n",
        "![LLM-as-a-Judge Evaluator](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGGlYrB7foSR",
        "outputId": "3e2d7d06-a5be-4552-9f17-88d93dd7b600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14:05:34.735 OpenAI Agents trace: Agent workflow\n",
            "14:05:34.736   Agent run: 'WebSearchAgent'\n",
            "14:05:34.738     Responses API with 'gpt-4o'\n"
          ]
        }
      ],
      "source": [
        "# Example: Checking if the agentâ€™s output is toxic or not.\n",
        "from agents import Agent, Runner, WebSearchTool\n",
        "\n",
        "# Define your agent with the web search tool\n",
        "agent = Agent(\n",
        "    name=\"WebSearchAgent\",\n",
        "    instructions=\"You are an agent that can search the web.\",\n",
        "    tools=[WebSearchTool()]\n",
        ")\n",
        "\n",
        "input_query = \"Is eating carrots good for the eyes?\"\n",
        "\n",
        "# Run agent\n",
        "with trace.get_tracer(__name__).start_as_current_span(\"OpenAI-Agent-Trace\") as span:\n",
        "    # Run your agent with a query\n",
        "    result = Runner.run_sync(agent, input_query)\n",
        "\n",
        "    # Add input and output values to parent trace\n",
        "    span.set_attribute(\"input.value\", input_query)\n",
        "    span.set_attribute(\"output.value\", result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izr-3LiQfoSR"
      },
      "source": [
        "You can see that the answer of this example is judged as \"not toxic\".\n",
        "\n",
        "![LLM-as-a-Judge Evaluation Score](https://langfuse.com/images/cookbook/integration_openai-agents/llm-as-a-judge-score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7fN0UTkfoSR"
      },
      "source": [
        "#### 6. Observability Metrics Overview\n",
        "\n",
        "All of these metrics can be visualized together in dashboards. This enables you to quickly see how your agent performs across many sessions and helps you to track quality metrics over time.\n",
        "\n",
        "![Observability metrics overview](https://langfuse.com/images/cookbook/integration_openai-agents/dashboard-dark.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlwltgEkfoSR"
      },
      "source": [
        "## Offline Evaluation\n",
        "\n",
        "Online evaluation is essential for live feedback, but you also need **offline evaluation**â€”systematic checks before or during development. This helps maintain quality and reliability before rolling changes into production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5R8eNQxfoSR"
      },
      "source": [
        "### Dataset Evaluation\n",
        "\n",
        "In offline evaluation, you typically:\n",
        "1. Have a benchmark dataset (with prompt and expected output pairs)\n",
        "2. Run your agent on that dataset\n",
        "3. Compare outputs to the expected results or use an additional scoring mechanism\n",
        "\n",
        "Below, we demonstrate this approach with the [search-dataset](https://huggingface.co/datasets/junzhang1207/search-dataset), which contains questions that can be answered via the web search tool and expected answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "dbf84b0798e0432599453e370740acaa",
            "f05fbf51e518417ea66a24db7b0a472e",
            "27794498f97b40a6b4ef1f2e36bf317e",
            "35fb6e8a89194653abcaa84f6da4f190",
            "93572e5407ea443999d1da45280741f7",
            "f3574545791843998157f0a3176e0ded",
            "636090e9932f4ff6a76152c64c92347d",
            "7928d3e09417467aaa74cb2c6cea32ca",
            "1ef6f4bf46e24841916cc9c611c1498b",
            "07f767145c7741bfb950cd983c757cc7",
            "b620b045db0242189200a99090ea6b9f",
            "33f6af1d99a9451a86e6e6690cec7e43",
            "722784d0cb184f02b5250c57829cfdc4",
            "e98c3c9567334b69a46ab23cd378358e",
            "f0c04648902343288b4248dbf5589d7b",
            "b32d7f992f064016ab548a961569b632",
            "a95842a82d46492796b7d0fd45bb9795",
            "9b33b4e7c6bb4a728f21258c10034066",
            "6b2cbe08ebad4df8bf9e4711d78920f2",
            "ed53bc55a6da404bac64993433f78ccf",
            "f68c9919bd26417cb1f3950f121276e3",
            "e815dd583c3243efa5d3b57672519f74",
            "ea5cea15ae5741418720f77d5879ecc2",
            "f8be1fdbe50649ebb614194c140e0d9a",
            "bdb56cd2387e4e30a1bf82beecf481d1",
            "12b08dc8912c4bfc81c81524eabc7898",
            "57152a81a7a24410992ccce525f28af0",
            "799212d16c814bd696d57d0cdd35d9c7",
            "f6935aa898f544d2b4051ee259ec9e30",
            "a049a1b305ed4660bc48f81fe1c6c0a7",
            "9e40a549b3094b3892de5159dfe935ff",
            "15fa9808df56469db8cc623bd127ceae",
            "8948024991964982ad058f199066cc55",
            "0f5356bc1d4a4e0f895fa9482bb06a7a",
            "427363e7b1ce4c7387b93f2794bdedb4",
            "690c3eb1f314478083b725bcb1d38a25",
            "edc537f5b13348c6b831365930c0cf31",
            "7f0d7cf658054c448668bebacd96a59d",
            "71277158b5ec43c8aa0accb86b15952d",
            "c6b284ea83ee4de0a6de5210067b4b6b",
            "c248d2dba9484807bc53d23ece012644",
            "2b4fb4ce8c71405d80da978589750950",
            "07431a8d1a2044b6baebcef3242f41c5",
            "e2941796709b4284aa3b6143b19e064e"
          ]
        },
        "id": "r77WUP9NfoSS",
        "outputId": "7809f9f1-b496-4672-e17c-41681e66accc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf84b0798e0432599453e370740acaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33f6af1d99a9451a86e6e6690cec7e43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data-samples.json:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea5cea15ae5741418720f77d5879ecc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data.jsonl:   0%|          | 0.00/316k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f5356bc1d4a4e0f895fa9482bb06a7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/934 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of GSM8K dataset:\n",
            "                                     id  \\\n",
            "0  20caf138-0c81-4ef9-be60-fe919e0d68d4   \n",
            "1  1f37d9fd-1bcc-4f79-b004-bc0e1e944033   \n",
            "2  76173a7f-d645-4e3e-8e0d-cca139e00ebe   \n",
            "3  5f5ef4ca-91fe-4610-a8a9-e15b12e3c803   \n",
            "4  64dbed0d-d91b-4acd-9a9c-0a7aa83115ec   \n",
            "\n",
            "                                            question  \\\n",
            "0                 steve jobs statue location budapst   \n",
            "1  Why is the Battle of Stalingrad considered a t...   \n",
            "2  In what year did 'The Birth of a Nation' surpa...   \n",
            "3  How many Russian soldiers surrendered to AFU i...   \n",
            "4   What event led to the creation of Google Images?   \n",
            "\n",
            "                                     expected_answer       category       area  \n",
            "0  The Steve Jobs statue is located in Budapest, ...           Arts  Knowledge  \n",
            "1  The Battle of Stalingrad is considered a turni...   General News       News  \n",
            "2  This question is based on a false premise. 'Th...  Entertainment       News  \n",
            "3  About 300 Russian soldiers surrendered to the ...   General News       News  \n",
            "4  Jennifer Lopez's appearance in a green Versace...     Technology       News  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Fetch search-dataset from Hugging Face\n",
        "dataset = load_dataset(\"junzhang1207/search-dataset\", split = \"train\")\n",
        "df = pd.DataFrame(dataset)\n",
        "print(\"First few rows of search-dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGnDwPySfoSS"
      },
      "source": [
        "Next, we create a dataset entity in Langfuse to track the runs. Then, we add each item from the dataset to the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDOHxyZRfoSS",
        "outputId": "f46eefc6-4c26-4f5b-cfcf-24b7887ed3f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(id='cm88w66t102qpad07xhgeyaej', name='search-dataset_huggingface_openai-agent', description='search-dataset uploaded from Huggingface', metadata={'date': '2025-03-14', 'type': 'benchmark'}, project_id='cloramnkj0002jz088vzn1ja4', created_at=datetime.datetime(2025, 3, 14, 14, 47, 14, 676000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 3, 14, 14, 47, 14, 676000, tzinfo=datetime.timezone.utc))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse import Langfuse\n",
        "langfuse = Langfuse()\n",
        "\n",
        "langfuse_dataset_name = \"search-dataset_huggingface_openai-agent\"\n",
        "\n",
        "# Create a dataset in Langfuse\n",
        "langfuse.create_dataset(\n",
        "    name=langfuse_dataset_name,\n",
        "    description=\"search-dataset uploaded from Huggingface\",\n",
        "    metadata={\n",
        "        \"date\": \"2025-03-14\",\n",
        "        \"type\": \"benchmark\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJy29tP0foSS"
      },
      "outputs": [],
      "source": [
        "for idx, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=langfuse_dataset_name,\n",
        "        input={\"text\": row[\"question\"]},\n",
        "        expected_output={\"text\": row[\"expected_answer\"]}\n",
        "    )\n",
        "    if idx >= 49: # For this example, we upload only the first 50 items\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgHw2e6afoSS"
      },
      "source": [
        "![Dataset items in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/example-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZl2W_u0foSS"
      },
      "source": [
        "#### Running the Agent on the Dataset\n",
        "\n",
        "We define a helper function `run_openai_agent()` that:\n",
        "1. Starts an OpenTelemetry span\n",
        "2. Runs our agent on the prompt\n",
        "3. Records the trace ID in Langfuse\n",
        "\n",
        "Then, we loop over each dataset item, run the agent, and link the trace to the dataset item. We can also attach a quick evaluation score if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rYh1PBRfoSS"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner, WebSearchTool\n",
        "from opentelemetry.trace import format_trace_id\n",
        "\n",
        "# Define your agent with the web search tool\n",
        "agent = Agent(\n",
        "    name=\"WebSearchAgent\",\n",
        "    instructions=\"You are an agent that can search the web.\",\n",
        "    tools=[WebSearchTool(search_context_size= \"high\")]\n",
        ")\n",
        "\n",
        "def run_openai_agent(question):\n",
        "    with tracer.start_as_current_span(\"OpenAI-Agent-Trace\") as span:\n",
        "        span.set_attribute(\"langfuse.tag\", \"dataset-run\")\n",
        "\n",
        "        # Run your agent with a query\n",
        "        result = Runner.run_sync(agent, question)\n",
        "\n",
        "        # Get the Langfuse trace_id to link the dataset run item to the agent trace\n",
        "        current_span = trace.get_current_span()\n",
        "        span_context = current_span.get_span_context()\n",
        "        trace_id = span_context.trace_id\n",
        "        formatted_trace_id = format_trace_id(trace_id)\n",
        "\n",
        "        langfuse_trace = langfuse.trace(\n",
        "            id=formatted_trace_id,\n",
        "            input=question,\n",
        "            output=result.final_output\n",
        "        )\n",
        "    return langfuse_trace, result.final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nF6JLCsYfoST",
        "outputId": "84c3b74e-1aa3-4ef2-f285-f390095b03c0"
      },
      "outputs": [],
      "source": [
        "dataset = langfuse.get_dataset(langfuse_dataset_name)\n",
        "\n",
        "# Run our agent against each dataset item\n",
        "for item in dataset.items:\n",
        "    langfuse_trace, output = run_openai_agent(item.input[\"text\"])\n",
        "\n",
        "    # Link the trace to the dataset item for analysis\n",
        "    item.link(\n",
        "        langfuse_trace,\n",
        "        run_name=\"openai-agent-run-03\",\n",
        "        run_metadata={ \"search_context_size\": \"high\"}\n",
        "    )\n",
        "\n",
        "    # Optionally, store a quick evaluation score for demonstration\n",
        "    langfuse_trace.score(\n",
        "        name=\"<example_eval>\",\n",
        "        value=1,\n",
        "        comment=\"This is a comment\"\n",
        "    )\n",
        "\n",
        "# Flush data to ensure all telemetry is sent\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUYV69HfoST"
      },
      "source": [
        "You can repeat this process with different:\n",
        "- Search tools (e.g. different context sized for OpenAI's `WebSearchTool`)\n",
        "- Models (gpt-4o-mini, o1, etc.)\n",
        "- Tools (search vs. no search)\n",
        "\n",
        "Then compare them side-by-side in Langfuse. In this example, I did run the agent 3 times on the 50 dataset questions. For each run, I used a different setting for the context size of OpenAI's `WebSearchTool`. You can see that an increased context size also slightly increased the answer correctness from `0.89` to `0.92`. The `correct_answer` score is created by an [LLM-as-a-Judge Evaluator](https://langfuse.com/docs/scores/model-based-evals) that is set up to judge the correctness of the question based on the sample answer given in the dataset.\n",
        "\n",
        "![Dataset run overview](https://langfuse.com/images/cookbook/integration_openai-agents/dataset_runs.png)\n",
        "![Dataset run comparison](https://langfuse.com/images/cookbook/integration_openai-agents/dataset-run-comparison.png)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07431a8d1a2044b6baebcef3242f41c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f767145c7741bfb950cd983c757cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5356bc1d4a4e0f895fa9482bb06a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_427363e7b1ce4c7387b93f2794bdedb4",
              "IPY_MODEL_690c3eb1f314478083b725bcb1d38a25",
              "IPY_MODEL_edc537f5b13348c6b831365930c0cf31"
            ],
            "layout": "IPY_MODEL_7f0d7cf658054c448668bebacd96a59d"
          }
        },
        "12b08dc8912c4bfc81c81524eabc7898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fa9808df56469db8cc623bd127ceae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8948024991964982ad058f199066cc55",
            "value": "â€‡316k/316kâ€‡[00:00&lt;00:00,â€‡1.94MB/s]"
          }
        },
        "15fa9808df56469db8cc623bd127ceae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef6f4bf46e24841916cc9c611c1498b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27794498f97b40a6b4ef1f2e36bf317e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7928d3e09417467aaa74cb2c6cea32ca",
            "max": 2125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ef6f4bf46e24841916cc9c611c1498b",
            "value": 2125
          }
        },
        "2b4fb4ce8c71405d80da978589750950": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33f6af1d99a9451a86e6e6690cec7e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_722784d0cb184f02b5250c57829cfdc4",
              "IPY_MODEL_e98c3c9567334b69a46ab23cd378358e",
              "IPY_MODEL_f0c04648902343288b4248dbf5589d7b"
            ],
            "layout": "IPY_MODEL_b32d7f992f064016ab548a961569b632"
          }
        },
        "35fb6e8a89194653abcaa84f6da4f190": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f767145c7741bfb950cd983c757cc7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b620b045db0242189200a99090ea6b9f",
            "value": "â€‡2.12k/2.12kâ€‡[00:00&lt;00:00,â€‡37.6kB/s]"
          }
        },
        "427363e7b1ce4c7387b93f2794bdedb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71277158b5ec43c8aa0accb86b15952d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c6b284ea83ee4de0a6de5210067b4b6b",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "57152a81a7a24410992ccce525f28af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2b1d2255a34a7597b263755eaa14b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636090e9932f4ff6a76152c64c92347d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "690c3eb1f314478083b725bcb1d38a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c248d2dba9484807bc53d23ece012644",
            "max": 934,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b4fb4ce8c71405d80da978589750950",
            "value": 934
          }
        },
        "6b2cbe08ebad4df8bf9e4711d78920f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71277158b5ec43c8aa0accb86b15952d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722784d0cb184f02b5250c57829cfdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95842a82d46492796b7d0fd45bb9795",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9b33b4e7c6bb4a728f21258c10034066",
            "value": "data-samples.json:â€‡100%"
          }
        },
        "7928d3e09417467aaa74cb2c6cea32ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799212d16c814bd696d57d0cdd35d9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0d7cf658054c448668bebacd96a59d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8948024991964982ad058f199066cc55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b10018448324153af2ee1f9bd83d140": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cebcce63ea37474ca10f1828105ca2e6",
              "IPY_MODEL_9153dfceabff450ead31493c3c518d4c"
            ],
            "layout": "IPY_MODEL_5a2b1d2255a34a7597b263755eaa14b3"
          }
        },
        "9153dfceabff450ead31493c3c518d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "ðŸ‘Ž",
            "disabled": false,
            "icon": "thumbs-down",
            "layout": "IPY_MODEL_ecd5521cdbc34eb7a866b4b2094fd500",
            "style": "IPY_MODEL_df007d6320cb4198a6dbf58485980394",
            "tooltip": ""
          }
        },
        "93572e5407ea443999d1da45280741f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b33b4e7c6bb4a728f21258c10034066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e40a549b3094b3892de5159dfe935ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a049a1b305ed4660bc48f81fe1c6c0a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a95842a82d46492796b7d0fd45bb9795": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32d7f992f064016ab548a961569b632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b620b045db0242189200a99090ea6b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdb56cd2387e4e30a1bf82beecf481d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a049a1b305ed4660bc48f81fe1c6c0a7",
            "max": 316103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e40a549b3094b3892de5159dfe935ff",
            "value": 316103
          }
        },
        "c248d2dba9484807bc53d23ece012644": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b284ea83ee4de0a6de5210067b4b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8b3aa3aeec046ef8acfab640c2dee17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebcce63ea37474ca10f1828105ca2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "ðŸ‘",
            "disabled": false,
            "icon": "thumbs-up",
            "layout": "IPY_MODEL_c8b3aa3aeec046ef8acfab640c2dee17",
            "style": "IPY_MODEL_ee1b1596e6ec42029fbf8b711c0fc41a",
            "tooltip": ""
          }
        },
        "dbf84b0798e0432599453e370740acaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f05fbf51e518417ea66a24db7b0a472e",
              "IPY_MODEL_27794498f97b40a6b4ef1f2e36bf317e",
              "IPY_MODEL_35fb6e8a89194653abcaa84f6da4f190"
            ],
            "layout": "IPY_MODEL_93572e5407ea443999d1da45280741f7"
          }
        },
        "df007d6320cb4198a6dbf58485980394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e2941796709b4284aa3b6143b19e064e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e815dd583c3243efa5d3b57672519f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98c3c9567334b69a46ab23cd378358e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2cbe08ebad4df8bf9e4711d78920f2",
            "max": 2479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed53bc55a6da404bac64993433f78ccf",
            "value": 2479
          }
        },
        "ea5cea15ae5741418720f77d5879ecc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8be1fdbe50649ebb614194c140e0d9a",
              "IPY_MODEL_bdb56cd2387e4e30a1bf82beecf481d1",
              "IPY_MODEL_12b08dc8912c4bfc81c81524eabc7898"
            ],
            "layout": "IPY_MODEL_57152a81a7a24410992ccce525f28af0"
          }
        },
        "ecd5521cdbc34eb7a866b4b2094fd500": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed53bc55a6da404bac64993433f78ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edc537f5b13348c6b831365930c0cf31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07431a8d1a2044b6baebcef3242f41c5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e2941796709b4284aa3b6143b19e064e",
            "value": "â€‡934/934â€‡[00:00&lt;00:00,â€‡33.25â€‡examples/s]"
          }
        },
        "ee1b1596e6ec42029fbf8b711c0fc41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f05fbf51e518417ea66a24db7b0a472e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3574545791843998157f0a3176e0ded",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_636090e9932f4ff6a76152c64c92347d",
            "value": "README.md:â€‡100%"
          }
        },
        "f0c04648902343288b4248dbf5589d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f68c9919bd26417cb1f3950f121276e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e815dd583c3243efa5d3b57672519f74",
            "value": "â€‡2.48k/2.48kâ€‡[00:00&lt;00:00,â€‡58.3kB/s]"
          }
        },
        "f3574545791843998157f0a3176e0ded": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68c9919bd26417cb1f3950f121276e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6935aa898f544d2b4051ee259ec9e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8be1fdbe50649ebb614194c140e0d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_799212d16c814bd696d57d0cdd35d9c7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f6935aa898f544d2b4051ee259ec9e30",
            "value": "data.jsonl:â€‡100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
